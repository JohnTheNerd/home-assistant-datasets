# serializer version: 1
# name: test_models
  list([
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'allenporter/assist-llm',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': 'Assist LLM fine tuned on Home Assistant Assist intents based on Llama 3.1 (8B)',
      'domain': 'ollama',
      'model_id': 'assist-llm',
      'rpm': None,
      'urls': list([
        'https://ollama.com/allenporter/assist-llm',
        'https://huggingface.co/allenporter/assist-llm',
        'https://huggingface.co/allenporter/assist-llm-GGUF',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-haiku-20240307',
        'llm_hass_api': 'assist',
      }),
      'description': 'Anthropic integration using Claude 3 Haiku',
      'domain': 'anthropic',
      'model_id': 'claude-3-haiku',
      'rpm': 25,
      'urls': list([
        'https://www.anthropic.com/news/claude-3-haiku',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'api_key': 'sk-0000000000000000000',
        'base_url': 'http://functionary.functionary:8000/v1',
      }),
      'config_entry_options': dict({
        'chat_model': 'functionary-small-v2.5',
        'llm_hass_api': 'assist',
      }),
      'description': 'A custom open AI integration using functionary small v2.5 (8B) with a modified pre-release llama cpp python server.',
      'domain': 'vicuna_conversation',
      'model_id': 'functionary-small-v2.5',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meetkai/functionary-small-v2.5',
        'https://github.com/abetlen/llama-cpp-python',
        'https://github.com/allenporter/functionary-server',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-pro',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini pro (v1)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-pro',
      'rpm': 30,
      'urls': list([
        'https://deepmind.google/technologies/gemini/pro/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'gemma:7b',
        'url': 'SECRET',
      }),
      'config_entry_options': None,
      'description': 'Gemma (7B) from Google using Ollama',
      'domain': 'ollama',
      'model_id': 'gemma',
      'rpm': None,
      'urls': list([
        'https://ollama.com/library/gemma',
        'https://ai.google.dev/gemma/docs/model_card',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-4o',
        'llm_hass_api': 'assist',
      }),
      'description': 'Open AI Conversation integration using gpt-4o',
      'domain': 'openai_conversation',
      'model_id': 'gpt-4o',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-4o',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'host': 'ollama.ollama',
        'huggingface_model': 'fixt/home-3b-v3:latest',
        'model_backend': 'ollama',
        'port': '11434',
        'ssl': False,
      }),
      'config_entry_options': dict({
        'context_length': 2048.0,
        'extra_attributes_to_expose': list([
          'rgb_color',
          'brightness',
          'temperature',
          'humidity',
          'fan_mode',
          'media_title',
          'volume_level',
          'item',
          'wind_speed',
        ]),
        'in_context_examples': False,
        'in_context_examples_file': 'in_context_examples.csv',
        'llm_hass_api': 'home-llm-service-api',
        'max_new_tokens': 128,
        'num_in_context_examples': 4.0,
        'ollama_json_mode': False,
        'ollama_keep_alive': 30.0,
        'prompt': '''
          You are 'Al', a helpful AI Assistant that controls the devices in a house. Complete the following task as instructed with the information provided only.
          The current time and date is {{ (as_timestamp(now()) | timestamp_custom("%I:%M %p on %A %B %d, %Y", "")) }}
          Services: {{ formatted_tools }}
          Devices:
          {{ formatted_devices }}
        ''',
        'prompt_template': 'zephyr',
        'refresh_prompt_per_turn': True,
        'remember_conversation': True,
        'remember_num_interactions': 5,
        'remote_use_chat_endpoint': False,
        'request_timeout': 90.0,
        'service_call_regex': '```homeassistant\\n([\\S \\t\\n]*?)```',
        'temperature': 0.1,
        'tool_format': 'min_tool_format',
        'tool_multi_turn_chat': False,
        'top_k': 40.0,
        'top_p': 1.0,
        'typical_p': 1.0,
      }),
      'description': 'The home-llm v3 model based on Phi (3B) and custom component using service calls to control Home Assistant.',
      'domain': 'llama_conversation',
      'model_id': 'home-llm',
      'rpm': None,
      'urls': list([
        'https://github.com/acon96/home-llm/',
        'https://huggingface.co/acon96/Home-3B-v3-GGUF',
        'https://ollama.com/fixt/home-3b-v3',
      ]),
      'version': 2,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'llama3-groq-tool-use:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': 'Groq tool use model fine tuned from llama3 (8B) using Ollama',
      'domain': 'ollama',
      'model_id': 'llama3-groq-tool-use',
      'rpm': None,
      'urls': list([
        'https://ollama.com/library/llama3-groq-tool-use',
        'https://console.groq.com/docs/tool-use',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'llama3:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': None,
      'description': 'Llama 3 (8B) from Meta using Ollama',
      'domain': 'ollama',
      'model_id': 'llama3',
      'rpm': None,
      'urls': list([
        'https://ollama.com/library/llama3',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'mistral:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': 'Mistral V3 (7B) using Ollama',
      'domain': 'ollama',
      'model_id': 'mistral-v3',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3',
        'https://ollama.com/library/mistral',
        'https://mistral.ai/news/announcing-mistral-7b/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'allenporter/xlam:1b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': 'XLam (1B) model from Salesforce using Ollama',
      'domain': 'ollama',
      'model_id': 'xlam-1b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/Salesforce/xLAM-1b-fc-r',
        'https://github.com/SalesforceAIResearch/xLAM',
        'https://ollama.com/allenporter/xlam:1b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
        'archived',
      ]),
      'config_entry_data': dict({
        'model': 'allenporter/xlam:7b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 4096,
      }),
      'description': 'XLam (7B) model from Salesforce using Ollama with 4k context window.',
      'domain': 'ollama',
      'model_id': 'xlam-7b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/Salesforce/xLAM-7b-fc-r',
        'https://github.com/SalesforceAIResearch/xLAM',
        'https://ollama.com/allenporter/xlam:7b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': None,
      'config_entry_options': None,
      'description': 'The Home Assisatnt NLP assistant pipeline',
      'domain': 'homeassistant',
      'model_id': 'assistant',
      'rpm': None,
      'urls': list([
        'https://github.com/home-assistant/hassil',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-5-haiku-20241022',
        'llm_hass_api': 'assist',
      }),
      'description': 'Anthropic integration using Claude 3.5 Haiku',
      'domain': 'anthropic',
      'model_id': 'claude-3-5-haiku',
      'rpm': 25,
      'urls': list([
        'https://www.anthropic.com/news/3-5-models-and-computer-use',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'claude-3-5-sonnet-20240620',
        'llm_hass_api': 'assist',
      }),
      'description': 'Anthropic integration using Claude 3.5 Sonnet',
      'domain': 'anthropic',
      'model_id': 'claude-3-5-sonnet',
      'rpm': 25,
      'urls': list([
        'https://www.anthropic.com/news/claude-3-5-sonnet',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-1.5-flash-latest',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini flash (v1.5)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-1.5-flash',
      'rpm': 100,
      'urls': list([
        'https://blog.google/products/gemini/google-gemini-new-features-july-2024/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-flash-lite',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini flash lite (v2.0) (exp)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-flash-lite',
      'rpm': 2000,
      'urls': list([
        'https://developers.googleblog.com/en/start-building-with-the-gemini-2-0-flash-family/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-flash',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini flash (v2.0)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-flash',
      'rpm': 1000,
      'urls': list([
        'https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.0-pro-exp-02-05',
        'llm_hass_api': 'assist',
      }),
      'description': 'Google Generative AI integration using gemini Pro (v2.0) (Exp)',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.0-pro',
      'rpm': 75,
      'urls': list([
        'https://blog.google/technology/google-deepmind/gemini-model-updates-february-2025/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'models/gemini-2.5-pro-preview-03-25',
        'llm_hass_api': 'assist',
      }),
      'description': '''
        Google Generative AI integration using Gemini 2.5, a thinking model, designed
        to tackle increasingly complex problems.
      ''',
      'domain': 'google_generative_ai_conversation',
      'model_id': 'gemini-2.5-pro',
      'rpm': 75,
      'urls': list([
        'https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'PetrosStav/gemma3-tools:27b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': '''
        Gemma3 is a lightweight variant of Gemini 2.0 models. This is a variant
        that has been customized to support tool use since the current ollama
        variant does not yet support tools.
  
      ''',
      'domain': 'ollama',
      'model_id': 'gemma3-27b',
      'rpm': None,
      'urls': list([
        'https://ollama.com/PetrosStav/gemma3-tools',
        'https://blog.google/technology/developers/gemma-3/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'PetrosStav/gemma3-tools:4b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': '''
        Gemma3 is a lightweight variant of Gemini 2.0 models. This is a variant
        that has been customized to support tool use since the current ollama
        variant does not yet support tools.
  
      ''',
      'domain': 'ollama',
      'model_id': 'gemma3-4b',
      'rpm': None,
      'urls': list([
        'https://ollama.com/PetrosStav/gemma3-tools',
        'https://blog.google/technology/developers/gemma-3/',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-3.5-turbo-0125',
        'llm_hass_api': 'assist',
      }),
      'description': 'Open AI Conversation integration using gpt-3.5 (175B)',
      'domain': 'openai_conversation',
      'model_id': 'gpt-3.5',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-3-5-turbo',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'SECRET',
      }),
      'config_entry_options': dict({
        'chat_model': 'gpt-4o-mini',
        'llm_hass_api': 'assist',
      }),
      'description': 'Open AI Conversation integration using gpt-4o-mini',
      'domain': 'openai_conversation',
      'model_id': 'gpt-4o-mini',
      'rpm': 250,
      'urls': list([
        'https://platform.openai.com/docs/models/gpt-4o-mini',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'llama3.1:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': 'Llama 3.1 (8B) from Meta using Ollama with 8k t window.',
      'domain': 'ollama',
      'model_id': 'llama3.1',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct',
        'https://ollama.com/library/llama3.1',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'llama3.2:1b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Meta's Llama 3.2 goes small with 1B and 3B models.",
      'domain': 'ollama',
      'model_id': 'llama3.2-1b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Llama-3.2-1B',
        'https://ollama.com/library/llama3.2:1b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'llama3.2:3b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Meta's Llama 3.2 goes small with 1B and 3B models.",
      'domain': 'ollama',
      'model_id': 'llama3.2-3b',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/meta-llama/Llama-3.2-3B',
        'https://ollama.com/library/llama3.2:3b',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'api_key': 'sk-0000000000000000000',
        'base_url': 'http://openai_api_replica:8000/v1',
      }),
      'config_entry_options': dict({
        'chat_model': 'llama-3.3-70b',
        'llm_hass_api': 'assist',
      }),
      'description': 'Llama 3.3 AWQ running in an OpenAI API-compatible server',
      'domain': 'vicuna_conversation',
      'model_id': 'llama3.3-awq',
      'rpm': None,
      'urls': list([
        'https://huggingface.co/casperhansen/llama-3.3-70b-instruct-awq',
        'https://huggingface.co/docs/transformers/en/quantization/awq',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'llama3.3:latest',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
      }),
      'description': 'Llama 3.3 (70B) from Meta using Ollama',
      'domain': 'ollama',
      'model_id': 'llama3.3',
      'rpm': None,
      'urls': list([
        'https://ollama.com/library/llama3.3',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'mistral-nemo',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': 'A state-of-the-art 12B model with 128k context length, built by Mistral AI in collaboration with NVIDIA.',
      'domain': 'ollama',
      'model_id': 'mistral-nemo',
      'rpm': None,
      'urls': list([
        'https://mistral.ai/news/mistral-nemo/',
        'https://ollama.com/library/mistral-nemo',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:14b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-14b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:32b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5-32b',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'qwen2.5:7b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      'domain': 'ollama',
      'model_id': 'qwen2.5',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwen2.5/',
        'https://ollama.com/library/qwen2.5',
      ]),
      'version': None,
    }),
    dict({
      'categories': list([
      ]),
      'config_entry_data': dict({
        'model': 'qwq:32b',
        'url': 'SECRET',
      }),
      'config_entry_options': dict({
        'llm_hass_api': 'assist',
        'num_ctx': 8192,
      }),
      'description': 'QwQ is the reasoning model of the Qwen series.',
      'domain': 'ollama',
      'model_id': 'qwq',
      'rpm': None,
      'urls': list([
        'https://qwenlm.github.io/blog/qwq-32b/',
        'https://ollama.com/library/qwq',
      ]),
      'version': None,
    }),
  ])
# ---
